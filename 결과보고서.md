# 궁디팡팡 AI 도슨트 - 결과 보고서

**제출자:** 김지원  
**제출일:** 2025년 12월 5일  
**과제:** (주)메쎄이상 웹 & AI 개발 부문 채용 과제  
**GitHub:** https://github.com/newkimjiwon/GDPP-AI-Docent-LLM

## 1. 기획 의도

### 1.1 과제 분석

메쎄이상의 궁디팡팡 캣페스타는 연간 10만 명 이상이 방문하는 국내 최대 고양이 박람회입니다. 방문객들은 다음과 같은 어려움을 겪습니다:

- **정보 접근성**: 243개 브랜드 중 원하는 제품을 찾기 어려움
- **부스 위치**: 넓은 전시장에서 특정 브랜드 부스를 찾기 어려움
- **시간 효율성**: 한정된 시간 내에 관심 있는 브랜드를 모두 방문하기 어려움

### 1.2 솔루션 제안

**"AI 도슨트"** - 박람회 전문 안내 챗봇

전시회 현장에서 방문객이 자연어로 질문하면, AI가 즉시 브랜드 정보, 부스 위치, 제품 정보를 안내하는 서비스를 기획했습니다.

**핵심 가치:**
1. **즉시성**: 대기 없이 즉시 정보 제공
2. **정확성**: RAG 기반으로 환각 없는 정확한 정보
3. **편의성**: 자연어 대화로 누구나 쉽게 사용
4. **독립성**: 외부 API 의존 없는 로컬 시스템

### 1.3 차별화 포인트

**일반적인 챗봇 vs 본 프로젝트**

| 구분 | 일반 챗봇 | 본 프로젝트 |
| :--- | :--- | :--- |
| **LLM** | OpenAI API (유료, 외부 의존) | **EXAONE 3.0 (로컬, 무료)** |
| **검색** | 단순 키워드 | **Hybrid RAG (Vector + BM25)** |
| **한국어** | 범용 모델 | **한국어 특화 모델** |
| **정확도** | 환각 발생 가능 | **환각 방지 시스템** |
| **비용** | 사용량 기반 과금 | **초기 구축 후 무료** |


## 2. 사용 기술 스택

### 2.1 전체 아키텍처

```
[사용자]
   ↓
[React Frontend] ← JWT 인증
   ↓
[FastAPI Backend]
   ↓
[Hybrid Retriever] → [ChromaDB Vector DB] + [BM25 Keyword Search]
   ↓
[EXAONE 3.0 LLM] ← [Ollama Server]
   ↓
[응답 생성 + 출처 표시]
```

### 2.2 기술 스택 상세

**Frontend**
- **React 19** + **Vite 7**: 빠른 개발 환경 및 최적화된 빌드
- **Tailwind CSS 3.4**: 유틸리티 기반 스타일링으로 일관된 디자인
- **Zustand 5.0**: 경량 상태 관리 (Redux 대비 간결함)
- **Axios**: HTTP 클라이언트
- **React Router**: 클라이언트 사이드 라우팅

**Backend**
- **Python 3.10** + **FastAPI**: 비동기 처리로 LLM 대기 시 다른 요청 처리 가능
- **SQLite** + **SQLAlchemy**: 경량 DB로 사용자 및 대화 관리
- **JWT** + **bcrypt**: 안전한 인증 시스템

**AI/ML**
- **EXAONE 3.0 7.8B** (LG AI Research): 한국어 특화 LLM
- **Ko-SBERT** (`jhgan/ko-sbert-nli`): 한국어 임베딩 모델
- **ChromaDB**: 벡터 데이터베이스
- **BM25**: 키워드 기반 검색
- **Ollama**: 로컬 LLM 서빙

**배포**
- **Docker Compose**: 프론트/백/LLM 서버 통합 관리
- **Nginx**: 프론트엔드 서빙
- **GCP e2-standard-8**: 클라우드 배포

### 2.3 기술 선택 이유

#### 왜 EXAONE 3.0인가?

**문제 상황:**
- 초기 Llama 3.1 8B 사용 → 한국어 응답 품질 저하
- Llama 3.2 3B로 경량화 시도 → 외국어(태국어 등) 혼입 발생
- 브랜드명, 부스 번호 등 정확한 정보 전달 필요

**해결:**
- EXAONE 3.0 7.8B (2024년 8월 출시)
- 한국어 문맥 이해도 우수
- 외국어 혼입 제거
- 국내 박람회 도메인에 최적화

**결과:**
- 부스 번호 검색 정확도 향상
- 외국어 혼입 완전 제거
- 자연스러운 한국어 응답

#### 왜 Hybrid Retrieval인가?

**문제 상황:**
- 단순 벡터 검색만 사용 시 "애니몬다" 같은 고유명사 검색 실패
- 의미는 이해하지만 정확한 브랜드명 매칭 어려움

**해결:**
- **Vector Search (ChromaDB)**: 의미 기반 검색
  - 예: "고양이 사료" → 관련 브랜드 검색
- **BM25 Search**: 키워드 정확 매칭
  - 예: "애니몬다" → Animonda 브랜드 정확 매칭
- 앙상블 가중치: Vector 70% + BM25 30%

**결과:**
- 검색 정확도 향상
- 고유명사 검색 성공률 개선



## 3. 구현 과정

### 3.1 Phase 1: 데이터 수집 및 전처리 (1일차)

**데이터 소스:**
1. **Wikipedia** (12개 페이지)
   - 고양이 관련 지식 (품종, 행동, 건강, 영양 등)
   - 총 크기: 36,911자
   
2. **GDPP 브랜드 정보** (243개)
   - 크롤링 대상: https://gdppcat.com/brand
   - 수집 정보: 브랜드명, 카테고리, 설명, 부스 번호, SNS 링크
   - 총 크기: 297,010자

3. **GDPP 이벤트 정보** (1개)
   - 2025년 일정 (4회차), 장소, 운영 시간
   - 사무국 연락처, 회사 정보, 중요 규칙
   - 총 크기: 1,139자

4. **GDPP FAQ** (39개)
   - 자주 묻는 질문 및 답변
   - 티켓 구매, 입장, 환불 등 실용 정보
   - 총 크기: 14,653자

**총 데이터 현황:**
- 총 원본 항목: **295개** (12 + 243 + 1 + 39)
- 총 청크: **297개** (전처리 후)
- 총 크기: **100,907자**
- 평균 청크 길이: **487자**

**전처리:**
```python
# Semantic Chunking (512 토큰 단위)
def chunk_brand_data(brand):
    text = f"""
브랜드명: {brand['brand_name']}
카테고리: {brand['category']}
설명: {brand['description']}
부스 번호: {brand['booth_number']}
"""
    return text
```

**결과:**
- 총 297개 청크 생성
- 평균 청크 길이: 487자
- 총 크기: 100,907자

### 3.2 Phase 2: RAG 시스템 구축 (2일차)

**벡터 DB 구축:**
```python
# Ko-SBERT로 임베딩 생성
embedder = KoSBERTEmbedder()
embeddings = embedder.embed_documents(chunks)

# ChromaDB에 저장
vector_store.add_documents(chunks, embeddings)
```

**Hybrid Retriever 구현:**
```python
class HybridRetriever:
    def search(self, query, top_k=5):
        # 1. Vector Search
        vector_results = self.vector_search(query, top_k)
        
        # 2. BM25 Search
        bm25_results = self.bm25_search(query, top_k)
        
        # 3. Ensemble (70% Vector + 30% BM25)
        final_results = self.ensemble(vector_results, bm25_results)
        
        return final_results
```

### 3.3 Phase 3: LLM 통합 및 프롬프트 엔지니어링 (3일차)

**시스템 프롬프트 설계:**
```python
SYSTEM_PROMPT = """당신은 궁디팡팡 캣페스타의 전문 AI 도슨트입니다.

중요한 규칙:
1. 제공된 [Context]에 있는 정보만 사용하세요.
2. 정보가 없으면 "죄송하지만 해당 정보가 없습니다"라고 답하세요.
3. 부스 번호, 브랜드명 등은 반드시 Context에서 확인 후 답변하세요.
4. 절대 정보를 지어내지 마세요.
"""
```

**환각 방지 시스템:**
1. Temperature 0.3 (보수적 응답)
2. 유사도 필터링 (임계값 0.15)
3. 출처 표시 강제

### 3.4 Phase 4: 백엔드 API 개발 (4일차)

**주요 엔드포인트:**
```python
# 채팅 API
@router.post("/api/chat")
async def chat(request: ChatRequest):
    # 1. Hybrid Search
    results = hybrid_retriever.search(query, top_k=5)
    
    # 2. 유사도 필터링
    filtered = [r for r in results if r['score'] >= 0.15]
    
    # 3. LLM 응답 생성
    response = await ollama_client.generate(
        prompt=create_prompt(query, filtered),
        temperature=0.3
    )
    
    return {"response": response, "sources": filtered}
```

**인증 시스템:**
- JWT 토큰 기반
- 비밀번호 bcrypt 해싱
- 게스트 모드 지원

### 3.5 Phase 5: 프론트엔드 개발 (5일차)

**주요 컴포넌트:**
1. **ChatArea**: 채팅 인터페이스
2. **Sidebar**: 대화 히스토리 관리
3. **ProductPanel**: 관심 상품 북마크
4. **Register**: 비밀번호 강도 검증

**비밀번호 강도 검증:**
```javascript
const checkPasswordStrength = (password) => {
    return {
        minLength: password.length >= 8,
        hasUpperCase: /[A-Z]/.test(password),
        hasLowerCase: /[a-z]/.test(password),
        hasNumber: /[0-9]/.test(password),
        hasSpecialChar: /[!@#$%^&*(),.?":{}|<>]/.test(password)
    };
};
```

### 3.6 Phase 6: Docker 배포 (6일차)

**docker-compose.yml:**
```yaml
services:
  frontend:  # React + Nginx
  backend:   # FastAPI
  ollama:    # LLM Server
```

**배포 명령어:**
```bash
docker compose up -d
docker exec -it gdpp-ollama ollama pull anpigon/exaone-3.0-7.8b-instruct-llamafied
docker exec -it gdpp-backend python src/rag/build_vectordb.py
docker compose restart backend
```



## 4. 개발 중 겪은 문제점과 해결 방안

### 4.1 문제 1: LLM 환각 현상

**문제:**
- 초기 Llama 3.1 모델 사용 시 없는 정보를 지어내는 환각 발생
- 부스 번호를 잘못 안내하거나 존재하지 않는 브랜드 언급

**원인 분석:**
- Temperature 0.7로 설정되어 창의적이지만 부정확한 응답 생성
- 관련성 낮은 문서도 Context에 포함
- 시스템 프롬프트가 약함

**해결 방안:**
1. **Temperature 조정**: 0.7 → 0.3
   ```python
   temperature = 0.3  # 더 보수적이고 사실 기반
   ```

2. **유사도 필터링 구현**:
   ```python
   SIMILARITY_THRESHOLD = 0.15
   filtered_results = [r for r in results if r['score'] >= SIMILARITY_THRESHOLD]
   ```

3. **시스템 프롬프트 강화**:
   ```python
   "절대 금지: Context에 없는 정보를 지어내는 것"
   ```

**결과:**
- 환각 현상 대폭 감소
- 부스 번호 정확도 향상

### 4.2 문제 2: 외국어 혼입

**문제:**
- Llama 3.2 3B 사용 시 한국어 응답에 태국어, 중국어 등 무관한 언어 혼입
- 예: "고양이 사료는 ที่ดีที่สุด..."

**원인 분석:**
- Llama 3.2 3B는 다국어 모델이지만 한국어 특화 부족
- 경량화 과정에서 언어 구분 능력 저하

**해결 방안:**
- **EXAONE 3.0 7.8B로 모델 교체**
  ```python
  model = "anpigon/exaone-3.0-7.8b-instruct-llamafied"
  ```
- LG AI Research의 한국어 특화 모델
- 2024년 8월 출시 최신 모델

**결과:**
- 외국어 혼입 완전 제거
- 자연스러운 한국어 응답
- 한국어 문맥 이해도 향상

### 4.3 문제 3: 브랜드명 검색 실패

**문제:**
- "애니몬다" 검색 시 "Animonda" 브랜드를 찾지 못함
- 한글/영어 표기 차이로 인한 검색 실패

**원인 분석:**
- Vector Search만 사용 시 의미는 이해하지만 정확한 매칭 실패
- 임베딩 모델이 "애니몬다"와 "Animonda"를 다른 단어로 인식

**해결 방안:**
- **Hybrid Retrieval 구현**
  ```python
  # Vector Search (의미 기반)
  vector_score = 0.7 * vector_similarity
  
  # BM25 Search (키워드 정확 매칭)
  bm25_score = 0.3 * bm25_similarity
  
  # Ensemble
  final_score = vector_score + bm25_score
  ```

**결과:**
- 고유명사 검색 성공률 개선
- 브랜드명 정확 매칭 가능

**향후 개선 방안:**
- 검색 키워드 필드 추가
  ```python
  search_keywords = "Animonda 애니몬다"
  ```
- 브랜드명 한글/영어 매핑 테이블 구축

### 4.4 문제 4: CPU 환경 응답 속도

**문제:**
- GCP CPU 환경에서 응답 시간 60-90초로 느림
- 사용자 경험 저하

**원인 분석:**
- 7.8B 모델은 GPU 가속 없이 CPU만으로 추론 시 느림
- 매 요청마다 모델 로딩/언로딩 발생

**해결 방안:**
1. **Ollama 최적화**:
   ```yaml
   environment:
     - OLLAMA_KEEP_ALIVE=-1  # 모델 영구 유지
     - OLLAMA_NUM_PARALLEL=4  # 병렬 처리
   ```

2. **사용자 경험 개선**:
   - 로딩 인디케이터 표시
   - 응답 생성 중 메시지 표시

**결과:**
- 첫 응답 후 모델이 메모리에 유지되어 후속 응답 빠름
- 사용자에게 진행 상황 피드백 제공

**권장 사항:**
- 실제 서비스 시 GPU 환경 사용 권장
- 또는 더 작은 모델 고려 (성능 트레이드오프)

### 4.5 문제 5: UI 겹침 문제

**문제:**
- 사이드바/제품 패널 접힘 시 채팅 메시지가 토글 버튼과 겹침

**원인 분석:**
- 고정 패딩으로 인해 동적 레이아웃 변화 미반영

**해결 방안:**
```javascript
// 동적 패딩 적용
<div className={`
    flex-1 overflow-y-auto p-4 
    ${!sidebarOpen ? 'pl-20' : ''} 
    ${!productPanelOpen ? 'pr-20' : ''}
`}>
```

**결과:**
- 패널 상태에 따라 자동으로 패딩 조정
- 깔끔한 UI 유지



## 5. 성능 및 검증

### 5.1 성능 지표

**LLM 응답 속도 (EXAONE 3.0 7.8B):**

| 환경 | 하드웨어 | 평균 응답 시간 | 토큰 생성 속도 |
| :--- | :--- | :--- | :--- |
| **Local GPU** | RTX 3090 24GB | 5-10초 | ~50 tokens/sec |
| **Cloud CPU** | GCP e2-standard-8 (8 vCPU, 32GB RAM) | 60-90초 | ~5-10 tokens/sec |

**검색 성능:**
- Hybrid Search 시간: ~100ms
- 검색 정확도: 관련 문서 포함률 높음

### 5.2 기능 검증

**테스트 시나리오:**
1. 브랜드 검색: "고양이 사료 추천해줘"
2. 부스 위치: "건강백서캣 부스 번호는?"
3. 이벤트 정보: "언제 열려?"
4. 고양이 지식: "페르시안 고양이에 대해 알려줘"
5. 없는 정보: "강아지 사료 있어?" → "해당 정보가 없습니다"

**결과:**
- 모든 시나리오 정상 작동
- 환각 없이 정확한 응답
- 출처 표시로 신뢰성 확보



## 6. 프로젝트 성과

### 6.1 기술적 성취

1. **로컬 LLM 기반 RAG 시스템 구축**
   - 외부 API 의존 없는 완전한 독립 시스템
   - 비용 절감 및 데이터 보안 확보

2. **한국어 특화 모델 적용**
   - EXAONE 3.0으로 한국어 품질 최적화
   - 외국어 혼입 제거

3. **환각 방지 시스템 구현**
   - 다층 방어 (프롬프트 + Temperature + 필터링)
   - 정확도 향상

4. **프로덕션 레벨 배포**
   - Docker Compose로 One-Command 배포
   - GCP 클라우드 배포 완료

### 6.2 비즈니스 가치

1. **방문객 경험 개선**
   - 즉시 정보 제공으로 대기 시간 제거
   - 자연어 대화로 접근성 향상

2. **운영 효율화**
   - 안내 인력 부담 감소
   - 24/7 자동 응답 가능

3. **확장 가능성**
   - 다른 박람회/전시회에 적용 가능
   - 데이터만 교체하면 재사용 가능

### 6.3 학습 성과

1. **최신 LLM 기술 습득**
   - EXAONE 3.0 활용 경험
   - RAG 시스템 구축 노하우

2. **문제 해결 능력**
   - 환각, 외국어 혼입 등 실제 문제 해결
   - 체계적인 디버깅 및 최적화

3. **풀스택 개발 역량**
   - React + FastAPI 통합
   - Docker 배포 경험



## 7. 향후 개선 계획

### 7.1 단기 개선 (1-2주)

1. **검색 키워드 최적화**
   - 브랜드명 한글/영어 매핑 추가
   - 검색 정확도 99% 목표

2. **성능 모니터링**
   - Prometheus + Grafana 연동
   - 응답 시간, 에러율 추적

### 7.2 중기 개선 (3-4주)

1. **이메일 인증**
   - SMTP 기반 이메일 인증
   - 비밀번호 재설정 기능

2. **다국어 지원**
   - 영어, 일본어 등 추가
   - 외국인 방문객 대응

### 7.3 장기 개선 (1-2개월)

1. **음성 인터페이스**
   - STT/TTS 통합
   - 핸즈프리 사용 가능

2. **추천 시스템**
   - 사용자 선호도 기반 브랜드 추천
   - 협업 필터링 적용



## 8. 결론

본 프로젝트는 메쎄이상의 궁디팡팡 캣페스타 방문객을 위한 AI 도슨트 챗봇으로, **로컬 LLM 기반 RAG 시스템**을 통해 정확하고 신뢰할 수 있는 정보를 제공합니다.

**핵심 성과:**
-  한국어 특화 EXAONE 3.0 모델 적용
-  Hybrid Retrieval로 검색 정확도 향상
-  환각 방지 시스템으로 신뢰성 확보
-  Docker One-Command 배포 완료
-  프로덕션 레벨 완성도

**기술적 차별화:**
- 대부분 지원자: OpenAI API 사용
- 본 프로젝트: 로컬 LLM + RAG 직접 구현

**비즈니스 가치:**
- 방문객 경험 개선
- 운영 효율화
- 확장 가능성

## 9. 실행화면

## EXAONE 3.0
<img width="1912" height="945" alt="EXAONE 3 0" src="https://github.com/user-attachments/assets/e9743ad1-e420-4c41-8bdd-3bef9eebf043" />

## llama3.1:8b
<img width="1915" height="944" alt="llama3 1_8b" src="https://github.com/user-attachments/assets/618f472e-a7fc-4921-b19e-88f747af4ff4" />

## llama3.2:3b
<img width="1914" height="946" alt="llama3 2_3b" src="https://github.com/user-attachments/assets/1b2a9b50-cd06-4e12-a5e5-9f76f7ca1aa9" />

## 메인화면
<img width="7796" height="3898" alt="기능_1" src="https://github.com/user-attachments/assets/04e1792f-0c19-48b8-a60d-5b4b75f60695" />

## 로그인
<img width="2555" height="1304" alt="image" src="https://github.com/user-attachments/assets/2bd3c3b7-0aef-43f1-8bec-5f7c5ad58031" />

## 회원가입
<img width="2553" height="1298" alt="image" src="https://github.com/user-attachments/assets/bb9f1936-d809-4cff-b556-c00b14334aa3" />

**제출 파일:**
- 소스 코드: https://github.com/newkimjiwon/GDPP-AI-Docent-LLM
- 실행 가이드: README.md
- 배포 가이드: DEPLOYMENT_GUIDE.md
- 결과 보고서: 본 문서

**감사합니다.**
